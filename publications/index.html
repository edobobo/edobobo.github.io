<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Edoardo  Barba | publications</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Edoardo</span>   Barba
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          <!-- resume -->
          <li class="nav-item">
            <a class="nav-link" href="/assets/pdf/Curriculum_Vitae.pdf">resume</a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">

  <h2 class="year">2025</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="zaporojets2025emergebenchmarkupdatingknowledge" class="col-sm-8">
    
      <div class="title">EMERGE: A Benchmark for Updating Knowledge Graphs with Emerging Textual Knowledge</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Zaporojets Klim,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Daza Daniel,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Assent Ira,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Navigli Roberto,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Groth Paul
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2025
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2507.03617" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="moroni-etal-2025-optimizing" class="col-sm-8">
    
      <div class="title">Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Moroni Luca,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Puccetti Giovanni,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Huguet Cabot Pere-Lluís,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bejgu Andrei Stefan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Miaschi Alessio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Dell’Orletta Felice,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Esuli Andrea,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Findings of the Association for Computational Linguistics: NAACL 2025</em>
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2025.findings-naacl.371.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/SapienzaNLP/sava" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token “fertility”) and slower inference speed.In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7B-v0.1, reducing token fertility by 25%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CLiC-it</abbr>
    
  
  </div>

  <div id="moroni-etal-2025-sustainable" class="col-sm-8">
    
      <div class="title">Sustainable Italian LLM Evaluation: Community Perspectives and Methodological Guidelines</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Moroni Luca,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Pappacoda Gianmarco,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Conia Simone,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Galassi Andrea,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Magnini Bernardo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Navigli Roberto,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Torroni Paolo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zanoli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Eleventh Italian Conference on Computational Linguistics (CLiC-it 2025)</em>
      
      
        2025
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="https://aclanthology.org/2025.clicit-1.71.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2024</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="perrella-etal-2024-beyond" class="col-sm-8">
    
      <div class="title">Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Perrella Stefano,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Proietti Lorenzo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Huguet Cabot Pere-Lluı́s,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2024.emnlp-main.1152.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/SapienzaNLP/guardians-mt-eval" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Machine Translation (MT) evaluation metrics assess translation quality automatically. Recently, researchers have employed MT metrics for various new use cases, such as data filtering and translation re-ranking. However, most MT metrics return assessments as scalar scores that are difficult to interpret, posing a challenge to making informed design choices. Moreover, MT metrics’ capabilities have historically been evaluated using correlation with human judgment, which, despite its efficacy, falls short of providing intuitive insights into metric performance, especially in terms of new metric use cases. To address these issues, we introduce an interpretable evaluation framework for MT metrics. Within this framework, we evaluate metrics in two scenarios that serve as proxies for the data filtering and translation re-ranking use cases. Furthermore, by measuring the performance of MT metrics using Precision, Recall, and F-score, we offer clearer insights into their capabilities than correlation with human judgments. Finally, we raise concerns regarding the reliability of manually curated data following the Direct Assessments+Scalar Quality Metrics (DA+SQM) guidelines, reporting a notably low agreement with Multidimensional Quality Metrics (MQM) annotations.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="orlando-etal-2024-relik" class="col-sm-8">
    
      <div class="title">ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Orlando Riccardo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Huguet Cabot Pere-Lluı́s,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Findings of the Association for Computational Linguistics ACL 2024</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2024.findings-acl.839.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/SapienzaNLP/relik" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in Natural Language Processing, serving as critical components in a wide range of applications. In this paper, we propose ReLiK, a Retriever-Reader architecture for both EL and RE, where, given an input text, the Retriever module undertakes the identification of candidate entities or relations that could potentially appear within the text. Subsequently, the Reader module is tasked to discern the pertinent retrieved entities or relations and establish their alignment with the corresponding textual spans. Notably, we put forward an innovative input representation that incorporates the candidate entities or relations alongside the text, making it possible to link entities or extract relations in a single forward pass and to fully leverage pre-trained language models contextualization capabilities, in contrast with previous Retriever-Reader-based methods, which require a forward pass for each candidate. Our formulation of EL and RE achieves state-of-the-art performance in both in-domain and out-of-domain benchmarks while using academic budget training and with up to 40x inference speed compared to competitors. Finally, we show how our architecture can be used seamlessly for Information Extraction (cIE), i.e. EL + RE, and setting a new state of the art by employing a shared Reader that simultaneously extracts entities and relations.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="perrella-etal-2024-guardians" class="col-sm-8">
    
      <div class="title">Guardians of the Machine Translation Meta-Evaluation: Sentinel Metrics Fall In!</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Perrella Stefano,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Proietti Lorenzo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Scirè Alessandro,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2024.acl-long.856.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/SapienzaNLP/guardians-mt-eval" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Annually, at the Conference of Machine Translation (WMT), the Metrics Shared Task organizers conduct the meta-evaluation of Machine Translation (MT) metrics, ranking them according to their correlation with human judgments. Their results guide researchers toward enhancing the next generation of metrics and MT systems. With the recent introduction of neural metrics, the field has witnessed notable advancements. Nevertheless, the inherent opacity of these metrics has posed substantial challenges to the meta-evaluation process. This work highlights two issues with the meta-evaluation framework currently employed in WMT, and assesses their impact on the metrics rankings. To do this, we introduce the concept of sentinel metrics, which are designed explicitly to scrutinize the meta-evaluation process’s accuracy, robustness, and fairness. By employing sentinel metrics, we aim to validate our findings, and shed light on and monitor the potential biases or inconsistencies in the rankings. We discover that the present meta-evaluation framework favors two categories of metrics: i) those explicitly trained to mimic human quality assessments, and ii) continuous metrics. Finally, we raise concerns regarding the evaluation capabilities of state-of-the-art metrics, emphasizing that they might be basing their assessments on spurious correlations found in their training data.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="martinelli-etal-2024-maverick" class="col-sm-8">
    
      <div class="title">Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Martinelli Giuliano,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2024.acl-long.722.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/SapienzaNLP/maverick-coref" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Large autoregressive generative models have emerged as the cornerstone for achieving the highest performance across several Natural Language Processing tasks. However, the urge to attain superior results has, at times, led to the premature replacement of carefully designed task-specific approaches without exhaustive experimentation. The Coreference Resolution task is no exception; all recent state-of-the-art solutions adopt large generative autoregressive models that outperform encoder-based discriminative systems. In this work, we challenge this recent trend by introducing Maverick, a carefully designed – yet simple – pipeline, which enables running a state-of-the-art Coreference Resolution system within the constraints of an academic budget, outperforming models with up to 13 billion parameters with as few as 500 million parameters. Maverick achieves state-of-the-art performance on the CoNLL-2012 benchmark, training with up to 0.006x the memory resources and obtaining a 170x faster inference compared to previous state-of-the-art systems. We extensively validate the robustness of the Maverick framework with an array of diverse experiments, reporting improvements over prior systems in data-scarce, long-document, and out-of-domain settings. We release our code and models for research purposes at https://github.com/SapienzaNLP/maverick-coref.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="bejgu-etal-2024-word" class="col-sm-8">
    
      <div class="title">Word Sense Linking: Disambiguating Outside the Sandbox</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Bejgu Andrei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Procopio Luigi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Fernández-Castro Alberte,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Findings of the Association for Computational Linguistics ACL 2024</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2024.findings-acl.851.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/Babelscape/WSL" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Word Sense Disambiguation (WSD) is the task of associating a word in a given context with its most suitable meaning among a set of possible candidates. While the task has recently witnessed renewed interest, with systems achieving performances above the estimated inter-annotator agreement, at the time of writing it still struggles to find downstream applications. We argue that one of the reasons behind this is the difficulty of applying WSD to plain text. Indeed, in the standard formulation, models work under the assumptions that a) all the spans to disambiguate have already been identified, and b) all the possible candidate senses of each span are provided, both of which are requirements that are far from trivial. In this work, we present a new task called Word Sense Linking (WSL) where, given an input text and a reference sense inventory, systems have to both identify which spans to disambiguate and then link them to their most suitable meaning.We put forward a transformer-based architecture for the task and thoroughly evaluate both its performance and those of state-of-the-art WSD systems scaled to WSL, iteratively relaxing the assumptions of WSD. We hope that our work will foster easier integration of lexical semantics into downstream applications.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="conia-etal-2024-mosaico" class="col-sm-8">
    
      <div class="title">MOSAICo: a Multilingual Open-text Semantically Annotated Interlinked Corpus</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Conia Simone,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Martinez Lorenzo Abelardo Carlos,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Huguet Cabot Pere-Lluı́s,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Orlando Riccardo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Procopio Luigi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2024.naacl-long.442.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/SapienzaNLP/mosaico" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Several Natural Language Understanding (NLU) tasks focus on linking text to explicit knowledge, including Word Sense Disambiguation, Semantic Role Labeling, Semantic Parsing, and Relation Extraction. In addition to the importance of connecting raw text with explicit knowledge bases, the integration of such carefully curated knowledge into deep learning models has been shown to be beneficial across a diverse range of applications, including Language Modeling and Machine Translation. Nevertheless, the scarcity of semantically-annotated corpora across various tasks and languages limits the potential advantages significantly. To address this issue, we put forward MOSAICo, the first endeavor aimed at equipping the research community with the key ingredients to model explicit semantic knowledge at a large scale, providing hundreds of millions of silver yet high-quality annotations for four NLU tasks across five languages. We describe the creation process of MOSAICo, demonstrate its quality and variety, and analyze the interplay between different types of semantic information. MOSAICo, available at https://github.com/SapienzaNLP/mosaico, aims to drop the requirement of closed, licensed datasets and represents a step towards a level playing field across languages and tasks in NLU.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="iyer-etal-2023-code" class="col-sm-8">
    
      <div class="title">Code-Switching with Word Senses for Pretraining in Neural Machine Translation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Iyer Vivek,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Birch Alexandra,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Pan Jeff,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Findings of the Association for Computational Linguistics: EMNLP 2023</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2023.findings-emnlp.859.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Lexical ambiguity is a significant and pervasive challenge in Neural Machine Translation (NMT), with many state-of-the-art (SOTA) NMT systems struggling to handle polysemous words (Campolungo et al., 2022). The same holds for the NMT pretraining paradigm of denoising synthetic “code-switched” text (Pan et al., 2021; Iyer et al., 2023), where word senses are ignored in the noising stage – leading to harmful sense biases in the pretraining data that are subsequently inherited by the resulting models. In this work, we introduce Word Sense Pretraining for Neural Machine Translation (WSP-NMT) - an end-to-end approach for pretraining multilingual NMT models leveraging word sense-specific information from Knowledge Bases. Our experiments show significant improvements in overall translation quality. Then, we show the robustness of our approach to scale to various challenging data and resource-scarce scenarios and, finally, report fine-grained accuracy improvements on the DiBiMT disambiguation benchmark. Our studies yield interesting and novel insights into the merits and challenges of integrating word sense information and structured knowledge in multilingual pretraining for NMT.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AACL</abbr>
    
  
  </div>

  <div id="martelli-etal-2023-lexicomatic" class="col-sm-8">
    
      <div class="title">LexicoMatic: Automatic Creation of Multilingual Lexical-Semantic Dictionaries</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Martelli Federico,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Procopio Luigi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2023.ijcnlp-main.53.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Lexical-semantic resources such as wordnets and multilingual dictionaries often suffer from significant coverage issues, especially in languages other than English. While improving their coverage manually is a prohibitively expensive undertaking, current approaches to the automatic creation of such resources fail to investigate the latest advances achieved in relevant fields, such as cross-lingual annotation projection. In this work, we address these shortcomings and propose LexicoMatic, a novel resource-independent approach to the automatic construction and expansion of multilingual semantic dictionaries, in which we formulate the task as an annotation projection problem. In addition, we tackle the lack of a comprehensive multilingual evaluation framework and put forward a new entirely manually-curated benchmark featuring 9 languages. We evaluate LexicoMatic with an extensive array of experiments and demonstrate the effectiveness of our approach, achieving a new state of the art across all languages under consideration. We release our novel evaluation benchmark at: https://github.com/SapienzaNLP/lexicomatic.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="barba-etal-2023-dmlm" class="col-sm-8">
    
      <div class="title">DMLM: Descriptive Masked Language Modeling</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Campolungo Niccolò,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Findings of the Association for Computational Linguistics: ACL 2023</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.diag.uniroma1.it/navigli/pubs/ACL_2023_Barbaetal.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Over the last few years, Masked Language Modeling (MLM) pre-training has resulted in remarkable advancements in many Natural Language Understanding (NLU) tasks, which sparked an interest in researching alternatives and extensions to the MLM objective. In this paper, we tackle the absence of explicit semantic grounding in MLM and propose Descriptive Masked Language Modeling (DMLM), a knowledge-enhanced reading comprehension objective, where the model is required to predict the most likely word in a context, being provided with the word’s definition. For instance, given the sentence “I was going to the _”, if we provided as definition “financial institution”, the model would have to predict the word “bank”; if, instead, we provided “sandy seashore”, the model should predict “beach”. Our evaluation highlights the effectiveness of DMLM in comparison with standard MLM, showing improvements on a number of well-established NLU benchmarks, as well as other semantics-focused tasks, e.g., Semantic Role Labeling. Furthermore, we demonstrate how it is possible to take full advantage of DMLM to embed explicit semantics in downstream tasks, explore several properties of DMLM-based contextual representations and suggest a number of future directions to investigate.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EACL</abbr>
    
  
  </div>

  <div id="procopio-etal-2023-entity" class="col-sm-8">
    
      <div class="title">Entity Disambiguation with Entity Definitions</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Procopio Luigi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Conia Simone,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2023.eacl-main.93.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/SapienzaNLP/extend" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Local models have recently attained astounding performances in Entity Disambiguation (ED), with generative and extractive formulations being the most promising research directions. However, previous works have so far limited their studies to using, as the textual representation of each candidate, only its Wikipedia title. Although certainly effective, this strategy presents a few critical issues, especially when titles are not sufficiently informative or distinguishable from one another. In this paper, we address this limitation and investigate the extent to which more expressive textual representations can mitigate it. We evaluate our approach thoroughly against standard benchmarks in ED and find extractive formulations to be particularly well-suited to such representations. We report a new state of the art on 2 out of the 6 benchmarks we consider and strongly improve the generalization capability over unseen patterns. We release our code, data and model checkpoints at https://github.com/SapienzaNLP/extend.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AACL</abbr>
    
  
  </div>

  <div id="navigli-etal-2022-tour" class="col-sm-8">
    
      <div class="title">A Tour of Explicit Multilingual Semantics: Word Sense Disambiguation, Semantic Role Labeling and Semantic Parsing</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Navigli Roberto,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Conia Simone,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Blloshmi Rexhina
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: Tutorial Abstracts</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The recent advent of modern pretrained language models has sparked a revolution in Natural Language Processing (NLP), especially in multilingual and cross-lingual applications. Today, such language models have become the de facto standard for providing rich input representations to neural systems, achieving unprecedented results in an increasing range of benchmarks. However, questions that often arise are: firstly, whether current language models are, indeed, able to capture explicit, symbolic meaning; secondly, if they are, to what extent; thirdly, and perhaps more importantly, whether current approaches are capable of scaling across languages. In this cutting-edge tutorial, we will review recent efforts that have aimed at shedding light on meaning in NLP, with a focus on three key open problems in lexical and sentence-level semantics: Word Sense Disambiguation, Semantic Role Labeling, and Semantic Parsing. After a brief introduction, we will spotlight how state-of-the-art models tackle these tasks in multiple languages, showing where they excel and where they fail. We hope that this tutorial will broaden the audience interested in multilingual semantics and inspire researchers to further advance the field.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="conia-etal-2022-dsrl" class="col-sm-8">
    
      <div class="title">Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Conia Simone,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Scirè Alessandro,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Findings of the Association for Computational Linguistics: EMNLP 2022</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>One of the common traits of past and present approaches for Semantic Role Labeling (SRL) is that they rely upon discrete labels drawn from a predefined linguistic inventory to classify predicate senses and their arguments.
However, we argue this need not be the case.
In this paper, we present an approach that leverages Definition Modeling to introduce a generalized formulation of SRL as the task of describing predicate-argument structures using natural language definitions instead of discrete labels.
Our novel formulation makes a first step towards placing interpretability and flexibility foremost, and yet our experiments and analyses on PropBank-style and FrameNet-style, dependency-based and span-based SRL demonstrate that a flexible model with an interpretable output does not necessarily come at the expense of performance.
We release our software and model checkpoints at https://anonymized.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="barba-etal-2022-extend" class="col-sm-8">
    
      <div class="title">ExtEnD: Extractive Entity Disambiguation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Procopio Luigi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2022.acl-long.177.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/SapienzaNLP/extend" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Local models for Entity Disambiguation (ED) have today become extremely powerful, in most part thanks to the advent of large pre-trained language models. However, despite their significant performance achievements, most of these approaches frame ED through classification formulations that have intrinsic limitations, both computationally and from a modeling perspective. In contrast with this trend, here we propose ExtEnD, a novel local formulation for ED where we frame this task as a text extraction problem, and present two Transformer-based architectures that implement it. Based on experiments in and out of domain, and training over two different data regimes, we find our approach surpasses all its competitors in terms of both data efficiency and raw performance. ExtEnD outperforms its alternatives by as few as 6 F1 points on the more constrained of the two data regimes and, when moving to the other higher-resourced regime, sets a new state of the art on 4 out of 4 benchmarks under consideration, with average improvements of 0.7 F1 points overall and 1.1 F1 points out of domain. In addition, to gain better insights from our results, we also perform a fine-grained evaluation of our performances on different classes of label frequency, along with an ablation study of our architectural choices and an error analysis. We release our code and models for research purposes at https://github.com/SapienzaNLP/extend.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
    
  
  </div>

  <div id="pepe-etal-2021-steps" class="col-sm-8">
    
      <div class="title">STEPS: Semantic Typing of Event Processes with a Sequence-to-Sequence Approach</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Pepe Sveva,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Blloshmi Rexhina,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of AAAI</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://github.com/SapienzaNLP/steps/blob/main/docs/AAAI22_STEPS.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/SapienzaNLP/steps" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Enabling computers to comprehend the intent of human actions by processing language is one of the fundamental goals of Natural Language Understanding. An emerging task in this context is that of free-form event process typing, which aims at understanding the overall goal of a protagonist in terms of an action and an object, given a sequence of events. This task was initially treated as a learning-to-rank problem by exploiting the similarity between processes and action/object textual definitions. However, this approach appears to be overly complex, binds the output types to a fixed inventory for possible word definitions and, moreover, leaves space for further enhancements as regards performance. In this paper, we advance the field by reformulating the free-form event process typing task as a sequence generation problem and put forward STEPS, an end-to-end approach for producing user intent in terms of actions and objects only, dispensing with the need for their definitions. In addition to this, we eliminate several dataset constraints set by previous works, while at the same time significantly outperforming them.  We release the data and software at https://github.com/SapienzaNLP/steps.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="barba-etal-2021-consec" class="col-sm-8">
    
      <div class="title">ConSeC: Word Sense Disambiguation as Continuous Sense Comprehension</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Procopio Luigi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2021.emnlp-main.112.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/SapienzaNLP/consec" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Supervised systems have nowadays become the standard recipe for Word Sense Disambiguation (WSD), with Transformer-based language models as their primary ingredient. However, while these systems have certainly attained unprecedented performances, virtually all of them operate under the constraining assumption that, given a context, each word can be disambiguated individually with no account of the other sense choices. To address this limitation and drop this assumption, we propose CONtinuous SEnse Comprehension (ConSeC), a novel approach to WSD: leveraging a recent re-framing of this task as a text extraction problem, we adapt it to our formulation and introduce a feedback loop strategy that allows the disambiguation of a target word to be conditioned not only on its context but also on the explicit senses assigned to nearby words. We evaluate ConSeC and examine how its components lead it to surpass all its competitors and set a new state of the art on English WSD. We also explore how ConSeC fares in the cross-lingual setting, focusing on 8 languages with various degrees of resource availability, and report significant improvements over prior systems. We release our code at https://github.com/SapienzaNLP/consec.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IJCAI</abbr>
    
  
  </div>

  <div id="ijcai2021-520" class="col-sm-8">
    
      <div class="title">Exemplification Modeling: Can You Give Me an Example, Please?</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Procopio Luigi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lacerra Caterina,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Pasini Tommaso,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, IJCAI-21</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.ijcai.org/proceedings/2021/0520.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recently, generative approaches have been used effectively to provide definitions of words in their context. However, the opposite, i.e., generating a usage example given one or more words along with their definitions, has not yet been investigated. In this work, we introduce the novel task of Exemplification Modeling (ExMod), along with a sequence-to-sequence architecture and a training procedure for it. Starting from a set of (word, definition) pairs, our approach is capable of automatically generating high-quality sentences which express the requested semantics. As a result, we can drive the creation of sense-tagged data which cover the full range of meanings in any inventory of interest, and their interactions within sentences. Human annotators agree that the sentences generated are as fluent and semantically-coherent with the input definitions as the sentences in manually-annotated corpora. Indeed, when employed as training data for Word Sense Disambiguation, our examples enable the current state of the art to be outperformed, and higher results to be achieved than when using gold-standard datasets only. We release the pretrained model, the dataset and the software at https://github.com/SapienzaNLP/exmod.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IJCAI</abbr>
    
  
  </div>

  <div id="ijcai2021-539" class="col-sm-8">
    
      <div class="title">MultiMirror: Neural Cross-lingual Word Alignment for Multilingual Word Sense Disambiguation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Procopio Luigi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Martelli Federico,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, IJCAI-21</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.ijcai.org/proceedings/2021/0539.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Word Sense Disambiguation (WSD), i.e., the task of assigning senses to words in context, has seen a surge of interest with the advent of neural models and a considerable increase in performance up to 80% F1 in English. However, when considering other languages, the availability of training data is limited, which hampers scaling WSD to many languages. To address this issue, we put forward MultiMirror, a sense projection approach for multilingual WSD based on a novel neural discriminative model for word alignment: given as input a pair of parallel sentences, our model – trained with a low number of instances – is capable of jointly aligning, at the same time, all source and target tokens with each other, surpassing its competitors across several language combinations. We demonstrate that projecting senses from English by leveraging the alignments produced by our model leads a simple mBERT-powered classifier to achieve a new state of the art on established WSD datasets in French, German, Italian, Spanish and Japanese. We release our software and all our datasets at https://github.com/SapienzaNLP/multimirror.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="barba-etal-2021-esc" class="col-sm-8">
    
      <div class="title">ESC: Redesigning WSD with Extractive Sense Comprehension</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Pasini Tommaso,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.aclweb.org/anthology/2021.naacl-main.371.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/SapienzaNLP/esc" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Word Sense Disambiguation (WSD) is a historical NLP task aimed at linking words in contexts to discrete sense inventories and it is usually cast as a multi-label classification task. Recently, several neural approaches have employed sense definitions to better represent word meanings. Yet, these approaches do not observe the input sentence and the sense definition candidates all at once, thus potentially reducing the model performance and generalization power. We cope with this issue by reframing WSD as a span extraction problem — which we called Extractive Sense Comprehension (ESC) — and propose ESCHER, a transformer-based neural architecture for this new formulation. By means of an extensive array of experiments, we show that ESC unleashes the full potential of our model, leading it to outdo all of its competitors and to set a new state of the art on the English WSD task. In the few-shot scenario, ESCHER proves to exploit training data efficiently, attaining the same performance as its closest competitor while relying on almost three times fewer annotations. Furthermore, ESCHER can nimbly combine data annotated with senses from different lexical resources, achieving performances that were previously out of everyone’s reach. The model along with data is available at https://github.com/SapienzaNLP/esc.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IJCAI</abbr>
    
  
  </div>

  <div id="ijcai2020-531" class="col-sm-8">
    
      <div class="title">MuLaN: Multilingual Label propagatioN for Word Sense Disambiguation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Barba Edoardo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Procopio Luigi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Campolungo Niccolò,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Pasini Tommaso,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Navigli Roberto
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, IJCAI-20</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.ijcai.org/Proceedings/2020/0531.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/SapienzaNLP/mulan" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      <a href="https://sapienzanlp.github.io/mulan/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The knowledge acquisition bottleneck strongly affects the creation of multilingual sense-annotated data, hence limiting the power of supervised systems when applied to multilingual Word Sense Disambiguation. In this paper, we propose a semi-supervised approach based upon a novel label propagation scheme, which, by jointly leveraging contextualized word embeddings and the multilingual information enclosed in a knowledge base, projects sense labels from a high-resource language, i.e., English, to lower-resourced ones. Backed by several experiments, we provide empirical evidence that our automatically created datasets are of a higher quality than those generated by other competitors and lead a supervised model to achieve state-of-the-art performances in all multilingual Word Sense Disambiguation tasks. We make our datasets available for research purposes at https://github.com/SapienzaNLP/mulan.</p>
    </div>
    
  </div>
</div>
</li></ol>

</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2026 Edoardo  Barba.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
